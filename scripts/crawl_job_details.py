import os
import json
import time
import random
from captcha_handle import check_for_captcha
from tqdm import tqdm
from multiprocessing import Process, current_process
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# === Project base path ===
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
URL_LIST_PATH = os.path.join(BASE_DIR, "job_urls", "topcv_2025-05-08.txt")
DATA_DIR = os.path.join(BASE_DIR, "raw_data")
LOG_FILE = os.path.join(BASE_DIR, "crawled_jobs.txt")
ERROR_LOG_FILE = os.path.join(BASE_DIR, "error_jobs.txt")  # Th√™m file log l·ªói ri√™ng
COOKIES_DIR = os.path.join(BASE_DIR, "cookies")
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(COOKIES_DIR, exist_ok=True)

# === Read URLs ===
with open(URL_LIST_PATH, "r", encoding="utf-8") as f:
    job_urls = [line.strip() for line in f.readlines()]

if os.path.exists(LOG_FILE):
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        crawled_ids = set(line.strip() for line in f)
else:
    crawled_ids = set()


# === WebDriver config ===
def get_driver():
    options = Options()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument(f"--window-size={random.randint(1024, 1600)},{random.randint(700, 900)}")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--lang=vi-VN")

    # Th√™m c√°c t√πy ch·ªçn ƒë·ªÉ tr√°nh ph√°t hi·ªán bot t·ª´ Cloudflare
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-web-security")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--disable-web-security")

    # Th√™m t√πy ch·ªçn ƒë·ªÉ ngƒÉn t·ª± ƒë·ªông chuy·ªÉn h∆∞·ªõng
    options.add_experimental_option("prefs", {
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_settings.popups": 0,
        "profile.managed_default_content_settings.images": 1,
        "profile.default_content_setting_values.automatic_downloads": 1
    })

    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15"
    ]
    options.add_argument(f"user-agent={random.choice(user_agents)}")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    service = Service(executable_path="C:/driver/undetected_chromedriver.exe")  # ‚Üê d√πng b·∫£n patch s·∫µn
    driver = webdriver.Chrome(service=service, options=options)
    return driver


def simulate_user_behavior(driver):
    """M√¥ ph·ªèng h√†nh vi ng∆∞·ªùi d√πng ƒë·ªÉ tr√°nh ph√°t hi·ªán bot"""
    # L∆∞·ªõt trang v·ªõi t·ªëc ƒë·ªô ng·∫´u nhi√™n
    scroll_times = random.randint(2, 5)
    for _ in range(scroll_times):
        scroll_amount = random.randint(100, 500)
        driver.execute_script(f"window.scrollBy(0, {scroll_amount});")
        time.sleep(random.uniform(0.5, 1.2))

    # Di chuy·ªÉn chu·ªôt
    actions = ActionChains(driver)
    actions.move_by_offset(random.randint(5, 50), random.randint(5, 50)).perform()
    time.sleep(random.uniform(0.5, 1))

    # Th√™m m·ªôt s·ªë h√†nh vi ng∆∞·ªùi d√πng ng·∫´u nhi√™n
    if random.random() < 0.3:  # 30% kh·∫£ nƒÉng
        # L∆∞·ªõt l√™n ƒë·∫ßu trang
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(random.uniform(0.5, 1))

    if random.random() < 0.2:  # 20% kh·∫£ nƒÉng
        # Di chuy·ªÉn ƒë·∫øn m·ªôt ph·∫ßn t·ª≠ ng·∫´u nhi√™n
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, "h2, h3, p, div.box")
            if elements:
                element = random.choice(elements)
                actions = ActionChains(driver)
                actions.move_to_element(element).perform()
                time.sleep(random.uniform(0.3, 0.7))
        except:
            pass


def load_cookies(driver, proc_id):
    """T·∫£i cookies n·∫øu c√≥ s·∫µn"""
    cookies_file = os.path.join(COOKIES_DIR, f"process_{proc_id}_cookies.json")
    if os.path.exists(cookies_file):
        try:
            # Truy c·∫≠p trang ch·ªß tr∆∞·ªõc khi th√™m cookies
            driver.get("https://www.topcv.vn")
            time.sleep(2)

            with open(cookies_file, "r") as f:
                cookies = json.load(f)
                for cookie in cookies:
                    try:
                        driver.add_cookie(cookie)
                    except Exception as e:
                        pass
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è Process {proc_id}: L·ªói khi t·∫£i cookies: {e}")
    return False


def save_cookies(driver, proc_id):
    """L∆∞u cookies ƒë·ªÉ s·ª≠ d·ª•ng sau"""
    cookies_file = os.path.join(COOKIES_DIR, f"process_{proc_id}_cookies.json")
    try:
        cookies = driver.get_cookies()
        with open(cookies_file, "w") as f:
            json.dump(cookies, f)
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Process {proc_id}: L·ªói khi l∆∞u cookies: {e}")
    return False


def log_error(proc_id, url, error_message):
    """Ghi l·ªói v√†o file log ri√™ng v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin"""
    try:
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        with open(ERROR_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] Process {proc_id}: {url} - {error_message}\n")
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ ghi log l·ªói: {e}")


def crawl_range(start, end, proc_id):
    """Crawl m·ªôt d·∫£i c√°c URLs ƒë∆∞·ª£c ph√¢n c√¥ng"""
    driver = None  # Kh·ªüi t·∫°o driver ·ªü c·∫•p cao h∆°n ƒë·ªÉ t√°i s·ª≠ d·ª•ng
    captcha_count = 0  # ƒê·∫øm s·ªë l·∫ßn g·∫∑p CAPTCHA

    try:
        driver = get_driver()  # Kh·ªüi t·∫°o driver
        wait = WebDriverWait(driver, 10)

        # T·∫£i cookies n·∫øu c√≥
        load_cookies(driver, proc_id)

        with open(LOG_FILE, "a", encoding="utf-8") as log_f:
            for index, url in enumerate(tqdm(job_urls[start:end], desc=f"üîÑ Process {proc_id}")):
                job_id = url.split("/")[-1].split("?")[0].split(".")[0]

                # URL ng·∫Øn ƒë·ªÉ hi·ªÉn th·ªã trong log (t·ªëi ƒëa 40 k√Ω t·ª±)
                short_url = url if len(url) <= 40 else f"{url[:37]}..."

                # Ki·ªÉm tra n·∫øu ƒë√£ crawl
                if job_id in crawled_ids:
                    continue

                # Ki·ªÉm tra n·∫øu file ƒë√£ t·ªìn t·∫°i
                file_path = os.path.join(DATA_DIR, f"{job_id}.json")
                if os.path.exists(file_path):
                    continue

                try:
                    # Truy c·∫≠p URL
                    driver.get(url)
                    original_url = driver.current_url  # L∆∞u URL g·ªëc
                    time.sleep(random.uniform(2, 3))  # ƒê·ª£i trang t·∫£i

                    # Ki·ªÉm tra CAPTCHA
                    if not check_for_captcha(driver, f"process-{proc_id}-job-{job_id}", wait_after_solve=15):
                        captcha_count += 1
                        error_msg = f"Kh√¥ng th·ªÉ x·ª≠ l√Ω CAPTCHA cho job {job_id}"
                        tqdm.write(f"‚ö†Ô∏è Process {proc_id}: {error_msg} - URL: {url}")
                        log_error(proc_id, url, error_msg)
                        continue  # B·ªè qua job n√†y v√† chuy·ªÉn sang job ti·∫øp theo

                    # Ki·ªÉm tra xem URL c√≥ thay ƒë·ªïi kh√¥ng sau khi x·ª≠ l√Ω captcha
                    if driver.current_url != original_url and "cloudflare" not in driver.current_url:
                        tqdm.write(
                            f"‚ö†Ô∏è Process {proc_id}: URL ƒë√£ thay ƒë·ªïi sau khi x·ª≠ l√Ω CAPTCHA. ƒêang quay l·∫°i URL g·ªëc...")
                        driver.get(url)
                        time.sleep(5)  # ƒê·ª£i l√¢u h∆°n ƒë·ªÉ trang t·∫£i

                    # Ki·ªÉm tra CAPTCHA
                    if not check_for_captcha(driver, f"process-{proc_id}-job-{job_id}"):
                        captcha_count += 1
                        error_msg = f"Kh√¥ng th·ªÉ x·ª≠ l√Ω CAPTCHA cho job {job_id}"
                        tqdm.write(f"‚ö†Ô∏è Process {proc_id}: {error_msg} - URL: {url}")
                        log_error(proc_id, url, error_msg)

                        # N·∫øu g·∫∑p nhi·ªÅu CAPTCHA li√™n ti·∫øp, kh·ªüi ƒë·ªông l·∫°i driver
                        if captcha_count >= 2:
                            restart_msg = f"Kh·ªüi ƒë·ªông l·∫°i driver sau {captcha_count} l·∫ßn g·∫∑p CAPTCHA"
                            tqdm.write(f"üîÑ Process {proc_id}: {restart_msg}")
                            # L∆∞u cookies tr∆∞·ªõc khi kh·ªüi ƒë·ªông l·∫°i
                            save_cookies(driver, proc_id)
                            driver.quit()
                            time.sleep(random.uniform(10, 15))  # ƒê·ª£i l√¢u h∆°n tr∆∞·ªõc khi kh·ªüi ƒë·ªông l·∫°i
                            driver = get_driver()
                            wait = WebDriverWait(driver, 10)
                            load_cookies(driver, proc_id)  # T·∫£i l·∫°i cookies
                            captcha_count = 0  # Reset b·ªô ƒë·∫øm

                        continue  # B·ªè qua job n√†y v√† chuy·ªÉn sang job ti·∫øp theo

                    captcha_count = 0  # Reset b·ªô ƒë·∫øm n·∫øu kh√¥ng g·∫∑p CAPTCHA

                    # M√¥ ph·ªèng h√†nh vi ng∆∞·ªùi d√πng
                    simulate_user_behavior(driver)

                    # L·∫•y d·ªØ li·ªáu t·ª´ trang
                    try:
                        # S·ª≠ d·ª•ng try-except cho t·ª´ng ph·∫ßn t·ª≠ ƒë·ªÉ kh√¥ng b·ªã l·ªói to√†n b·ªô job n·∫øu 1 ph·∫ßn b·ªã thi·∫øu
                        job_data = {"url": url}

                        try:
                            title = wait.until(
                                EC.presence_of_element_located((By.CSS_SELECTOR, "h1.job-detail__info--title"))).text
                            job_data["title"] = title
                        except Exception as e:
                            error_msg = f"Kh√¥ng l·∫•y ƒë∆∞·ª£c ti√™u ƒë·ªÅ job - l·ªói: {str(e)}"
                            tqdm.write(f"‚ö†Ô∏è Process {proc_id}: {error_msg} - URL: {url}")
                            log_error(proc_id, url, error_msg)
                            job_data["title"] = ""

                        try:
                            salary = driver.find_element(By.CSS_SELECTOR,
                                                         ".job-detail__info--section:nth-child(1) .job-detail__info--section-content-value").text
                            job_data["salary"] = salary
                        except Exception as e:
                            log_error(proc_id, url, f"Kh√¥ng l·∫•y ƒë∆∞·ª£c salary - l·ªói: {str(e)}")
                            job_data["salary"] = "Kh√¥ng c√¥ng b·ªë"

                        try:
                            location = driver.find_element(By.CSS_SELECTOR,
                                                           ".job-detail__info--section:nth-child(2) .job-detail__info--section-content-value").text
                            job_data["location"] = location
                        except Exception as e:
                            log_error(proc_id, url, f"Kh√¥ng l·∫•y ƒë∆∞·ª£c location - l·ªói: {str(e)}")
                            job_data["location"] = ""

                        try:
                            experience = driver.find_element(By.CSS_SELECTOR,
                                                             ".job-detail__info--section:nth-child(3) .job-detail__info--section-content-value").text
                            job_data["experience"] = experience
                        except Exception as e:
                            log_error(proc_id, url, f"Kh√¥ng l·∫•y ƒë∆∞·ª£c experience - l·ªói: {str(e)}")
                            job_data["experience"] = ""

                        try:
                            company_name = driver.find_element(By.CSS_SELECTOR, ".company-name-label").text
                            job_data["company_name"] = company_name
                        except Exception as e:
                            log_error(proc_id, url, f"Kh√¥ng l·∫•y ƒë∆∞·ª£c company_name - l·ªói: {str(e)}")
                            job_data["company_name"] = ""

                        try:
                            job_description = driver.find_element(By.CSS_SELECTOR,
                                                                  ".job-description__item:nth-child(1) .job-description__item--content").text
                            job_data["job_description"] = job_description
                        except Exception as e:
                            log_error(proc_id, url, f"Kh√¥ng l·∫•y ƒë∆∞·ª£c job_description - l·ªói: {str(e)}")
                            job_data["job_description"] = ""

                        try:
                            requirements = driver.find_element(By.CSS_SELECTOR,
                                                               ".job-description__item:nth-child(2) .job-description__item--content").text
                            job_data["requirements"] = requirements
                        except Exception as e:
                            log_error(proc_id, url, f"Kh√¥ng l·∫•y ƒë∆∞·ª£c requirements - l·ªói: {str(e)}")
                            job_data["requirements"] = ""

                        try:
                            tags = driver.find_elements(By.CSS_SELECTOR, ".box-category-tags > .box-category-tag")
                            career_tags = [tag.text for tag in tags]
                            job_data["career_tags"] = career_tags
                        except Exception as e:
                            log_error(proc_id, url, f"Kh√¥ng l·∫•y ƒë∆∞·ª£c career_tags - l·ªói: {str(e)}")
                            job_data["career_tags"] = []

                        # Th√™m timestamp
                        job_data["crawled_at"] = time.strftime("%Y-%m-%d %H:%M:%S")

                        # Ki·ªÉm tra xem c√≥ ƒë·ªß d·ªØ li·ªáu c·∫ßn thi·∫øt kh√¥ng
                        required_fields = ["title", "company_name", "job_description"]
                        missing_fields = [field for field in required_fields if not job_data.get(field)]

                        if missing_fields:
                            error_msg = f"Thi·∫øu c√°c tr∆∞·ªùng quan tr·ªçng: {', '.join(missing_fields)}"
                            tqdm.write(f"‚ö†Ô∏è Process {proc_id}: {error_msg} - URL: {url}")
                            log_error(proc_id, url, error_msg)
                            # V·∫´n l∆∞u d·ªØ li·ªáu b·∫•t k·ªÉ thi·∫øu

                        # L∆∞u d·ªØ li·ªáu v√†o file
                        with open(file_path, "w", encoding="utf-8") as f:
                            json.dump(job_data, f, ensure_ascii=False, indent=2)

                        # Ghi log
                        log_f.write(f"{job_id}\n")
                        log_f.flush()

                        # Hi·ªÉn th·ªã th√¥ng b√°o th√†nh c√¥ng v·ªõi URL ƒë·∫ßy ƒë·ªß
                        tqdm.write(f"‚úÖ Process {proc_id}: ƒê√£ l∆∞u job {job_id} - URL: {url}")

                    except Exception as e:
                        error_msg = f"L·ªói khi crawl job: {str(e)}"
                        tqdm.write(f"‚ùå Process {proc_id}: {error_msg} - URL: {url}")
                        log_error(proc_id, url, error_msg)

                except Exception as e:
                    error_msg = f"L·ªói truy c·∫≠p URL: {str(e)}"
                    tqdm.write(f"‚ùå Process {proc_id}: {error_msg} - URL: {url}")
                    log_error(proc_id, url, error_msg)

                # ƒê·ª£i m·ªôt kho·∫£ng th·ªùi gian ng·∫´u nhi√™n tr∆∞·ªõc khi ti·∫øp t·ª•c
                time.sleep(random.uniform(3.5, 7.5))

                # L∆∞u cookies ƒë·ªãnh k·ª≥
                if (index + 1) % 10 == 0:
                    save_cookies(driver, proc_id)

                # Kh·ªüi ƒë·ªông l·∫°i driver sau m·ªôt s·ªë l·∫ßn nh·∫•t ƒë·ªãnh
                if (index + 1) % 20 == 0:
                    tqdm.write(f"üîÑ Process {proc_id}: Kh·ªüi ƒë·ªông l·∫°i driver sau 20 jobs")
                    save_cookies(driver, proc_id)  # L∆∞u cookies tr∆∞·ªõc khi kh·ªüi ƒë·ªông l·∫°i
                    driver.quit()
                    time.sleep(random.uniform(5, 10))
                    driver = get_driver()
                    wait = WebDriverWait(driver, 10)
                    load_cookies(driver, proc_id)  # T·∫£i l·∫°i cookies

    except Exception as e:
        error_msg = f"Process g·∫∑p l·ªói nghi√™m tr·ªçng: {str(e)}"
        print(f"‚ùå Process {proc_id}: {error_msg}")
        log_error(proc_id, "global_error", error_msg)
    finally:
        if driver:
            # L∆∞u cookies tr∆∞·ªõc khi tho√°t
            save_cookies(driver, proc_id)
            driver.quit()


if __name__ == "__main__":
    # Hi·ªÉn th·ªã th√¥ng tin
    print(f"üìä T·ªïng s·ªë URLs c·∫ßn crawl: {len(job_urls)}")
    print(f"üìä ƒê√£ crawl tr∆∞·ªõc ƒë√≥: {len(crawled_ids)}")
    print(f"üìä C√≤n l·∫°i: {len(job_urls) - len(crawled_ids)}")

    # Kh·ªüi t·∫°o log file l·ªói n·∫øu ch∆∞a t·ªìn t·∫°i
    if not os.path.exists(ERROR_LOG_FILE):
        with open(ERROR_LOG_FILE, "w", encoding="utf-8") as f:
            f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] B·∫Øt ƒë·∫ßu log l·ªói\n")

    # T√πy ch·ªânh s·ªë l∆∞·ª£ng processes
    num_processes = 4  # C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh d·ª±a tr√™n c·∫•u h√¨nh m√°y
    chunk_size = len(job_urls) // num_processes

    # Kh·ªüi t·∫°o c√°c processes
    processes = []

    # T·∫°o v√† kh·ªüi ƒë·ªông t·ª´ng process
    for i in range(num_processes):
        start = i * chunk_size
        end = (i + 1) * chunk_size if i != num_processes - 1 else len(job_urls)
        p = Process(target=crawl_range, args=(start, end, i))
        processes.append(p)
        p.start()
        time.sleep(2)  # ƒê·ª£i m·ªôt ch√∫t gi·ªØa c√°c process ƒë·ªÉ tr√°nh qu√° t·∫£i

    # ƒê·ª£i t·∫•t c·∫£ c√°c processes ho√†n th√†nh
    for p in processes:
        p.join()

    print("‚úÖ Ho√†n th√†nh crawl t·∫•t c·∫£ job URLs!")