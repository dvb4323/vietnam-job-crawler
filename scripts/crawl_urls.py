import time
import random
from datetime import datetime
import os
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import undetected_chromedriver as uc
from captcha_handle import check_for_captcha

def human_scroll(driver, scroll_pause=1.0):
    scroll_height = driver.execute_script("return document.body.scrollHeight")
    current_height = 0
    while current_height < scroll_height:
        scroll_amount = random.randint(250, 450)
        current_height += scroll_amount
        driver.execute_script(f"window.scrollTo(0, {current_height});")
        time.sleep(scroll_pause)


def random_sleep(min_sec=1, max_sec=2):
    time.sleep(random.uniform(min_sec, max_sec))


def init_driver():
    chrome_options = uc.ChromeOptions()
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("start-maximized")
    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.71 Safari/537.36")
    chrome_options.add_argument("--disable-popup-blocking")
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--ignore-certificate-errors")
    driver = uc.Chrome(options=chrome_options)
    return driver


def save_urls_to_file(urls, filepath):
    """
    LÆ°u danh sÃ¡ch URLs vÃ o file vá»›i xá»­ lÃ½ táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i

    Args:
        urls (list): Danh sÃ¡ch URLs cáº§n lÆ°u
        filepath (str): ÄÆ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n file
    """
    # Äáº£m báº£o thÆ° má»¥c tá»“n táº¡i
    directory = os.path.dirname(filepath)

    # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i (bao gá»“m cáº£ thÆ° má»¥c con)
    if directory and not os.path.exists(directory):
        try:
            os.makedirs(directory)
            print(f"âœ… ÄÃ£ táº¡o thÆ° má»¥c: {directory}")
        except Exception as e:
            print(f"âŒ KhÃ´ng thá»ƒ táº¡o thÆ° má»¥c {directory}: {e}")
            return False

    # LÆ°u URLs vÃ o file
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            for url in urls:
                f.write(url + "\n")
        return True
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u file {filepath}: {e}")
        return False


def crawl_urls(base_url, max_pages=50, output_path=None):
    driver = init_driver()
    wait = WebDriverWait(driver, 20)

    job_urls = []
    page = 1

    try:
        while page <= max_pages:
            url = f"{base_url}&page={page}"
            print(f"\nðŸ”Ž Crawling page {page}: {url}")

            # Táº£i trang vá»›i retry náº¿u lá»—i
            try:
                driver.get(url)
            except TimeoutException:
                print(f"â±ï¸ Timeout trang {page}, thá»­ láº¡i sau 5 giÃ¢y...")
                time.sleep(5)
                try:
                    driver.get(url)
                except Exception as e:
                    print(f"âš ï¸ Lá»—i tiáº¿p tá»¥c trang {page}: {e}")
                    page += 1
                    continue

            random_sleep(2, 3)

            # Kiá»ƒm tra CAPTCHA thÃ´ng qua iframe chá»©a Google reCAPTCHA
            # try:
            #     captcha_frame = driver.find_element(By.CSS_SELECTOR, "iframe[src*='recaptcha']")
            #     print(f"ðŸš¨ CAPTCHA phÃ¡t hiá»‡n trÃªn trang {page}.")
            #     input("ðŸ”“ Vui lÃ²ng xá»­ lÃ½ CAPTCHA thá»§ cÃ´ng, sau Ä‘Ã³ nháº¥n Enter Ä‘á»ƒ tiáº¿p tá»¥c...")
            # except NoSuchElementException:
            #     pass  # KhÃ´ng cÃ³ CAPTCHA, tiáº¿p tá»¥c bÃ¬nh thÆ°á»ng

            # Xá»­ lÃ½ CAPTCHA vá»›i hÃ m nÃ¢ng cao
            if not check_for_captcha(driver, page):
                print(f"âš ï¸ KhÃ´ng thá»ƒ xá»­ lÃ½ CAPTCHA trÃªn trang {page}, thá»­ trang tiáº¿p theo")
                page += 1
                continue

            human_scroll(driver)
            # ActionChains(driver).move_by_offset(random.randint(0, 150), random.randint(0, 150)).perform()
            try:
                element = driver.find_element(By.TAG_NAME, "body")
                ActionChains(driver).move_to_element(element).perform()
            except Exception as e:
                print(f"âš ï¸ KhÃ´ng thá»ƒ move_to_element: {e}")

            random_sleep(1, 2)

            try:
                job_cards = wait.until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h3.title a'))
                )
            except (NoSuchElementException, TimeoutException) as e:
                print(f"âŒ KhÃ´ng tÃ¬m tháº¥y job cards trÃªn trang {page}: {e}")
                page += 1
                continue

            print(f"ðŸ“Œ TÃ¬m tháº¥y {len(job_cards)} job URLs trÃªn trang {page}")
            for job_card in job_cards:
                job_url = job_card.get_attribute("href")
                if job_url and job_url not in job_urls:
                    job_urls.append(job_url)

            page += 1
            random_sleep(1, 2)

            # TÃ¡i khá»Ÿi Ä‘á»™ng trÃ¬nh duyá»‡t sau má»—i 10 trang
            if page % 10 == 0:
                print("ðŸ” Äang khá»Ÿi Ä‘á»™ng láº¡i trÃ¬nh duyá»‡t Ä‘á»ƒ trÃ¡nh crash...")
                driver.quit()
                driver = init_driver()
                wait = WebDriverWait(driver, 20)
                random_sleep(2, 3)

    except WebDriverException as e:
        print(f"âŒ WebDriverException: {e}")
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {e}")
    finally:
        try:
            driver.quit()
        except:
            pass

    # LÆ°u káº¿t quáº£
    print(f"\nðŸ“¦ Tá»•ng sá»‘ URL thu tháº­p Ä‘Æ°á»£c: {len(job_urls)}")

    if job_urls:
        # XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng dáº«n file Ä‘áº§u ra
        if output_path is None:
            today = datetime.today().strftime("%Y-%m-%d")
            # Sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i tá»« thÆ° má»¥c hiá»‡n táº¡i
            current_dir = os.path.dirname(os.path.abspath(__file__))
            parent_dir = os.path.dirname(current_dir)
            output_dir = os.path.join(parent_dir, "job_urls")
            output_path = os.path.join(output_dir, f"topcv_{today}.txt")

        # LÆ°u URLs vÃ o file
        success = save_urls_to_file(job_urls, output_path)
        if success:
            print(f"âœ… ÄÃ£ lÆ°u {len(job_urls)} URLs vÃ o '{output_path}'")
        else:
            print(f"âŒ KhÃ´ng thá»ƒ lÆ°u URLs vÃ o '{output_path}'")
    else:
        print("âš ï¸ KhÃ´ng cÃ³ URL nÃ o Ä‘Æ°á»£c lÆ°u.")

    return job_urls


if __name__ == "__main__":
    BASE_URL = "https://www.topcv.vn/tim-viec-lam-moi-nhat?type_keyword=0&sba=1"

    # Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i cá»§a thÆ° má»¥c hiá»‡n táº¡i
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c job_urls (cÃ¹ng cáº¥p vá»›i thÆ° má»¥c scripts)
    parent_dir = os.path.dirname(current_dir)

    # Táº¡o tÃªn file
    today = datetime.today().strftime("%Y-%m-%d")
    output_file = os.path.join(parent_dir, "job_urls", f"topcv_{today}.txt")

    # Crawl vÃ  lÆ°u URLs
    crawl_urls(BASE_URL, max_pages=200, output_path=output_file)