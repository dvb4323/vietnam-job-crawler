import time
import random
from datetime import datetime
import os
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import undetected_chromedriver as uc

def human_scroll(driver, scroll_pause=1.0):
    scroll_height = driver.execute_script("return document.body.scrollHeight")
    current_height = 0
    while current_height < scroll_height:
        scroll_amount = random.randint(250, 450)
        current_height += scroll_amount
        driver.execute_script(f"window.scrollTo(0, {current_height});")
        time.sleep(scroll_pause)

def random_sleep(min_sec=1, max_sec=2):
    time.sleep(random.uniform(min_sec, max_sec))

def init_driver():
    chrome_options = uc.ChromeOptions()
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("start-maximized")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.71 Safari/537.36")
    chrome_options.add_argument("--disable-popup-blocking")
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--ignore-certificate-errors")
    driver = uc.Chrome(options=chrome_options)
    return driver

def crawl_urls(base_url, max_pages=50):
    driver = init_driver()
    wait = WebDriverWait(driver, 20)

    job_urls = []
    page = 1

    try:
        while page <= max_pages:
            url = f"{base_url}&page={page}"
            print(f"\nðŸ”Ž Crawling page {page}: {url}")

            # Táº£i trang vá»›i retry náº¿u lá»—i
            try:
                driver.get(url)
            except TimeoutException:
                print(f"â±ï¸ Timeout trang {page}, thá»­ láº¡i sau 5 giÃ¢y...")
                time.sleep(5)
                try:
                    driver.get(url)
                except Exception as e:
                    print(f"âš ï¸ Lá»—i tiáº¿p tá»¥c trang {page}: {e}")
                    page += 1
                    continue

            random_sleep(2, 3)

            # Kiá»ƒm tra CAPTCHA thÃ´ng qua iframe chá»©a Google reCAPTCHA
            try:
                captcha_frame = driver.find_element(By.CSS_SELECTOR, "iframe[src*='recaptcha']")
                print(f"ðŸš¨ CAPTCHA phÃ¡t hiá»‡n trÃªn trang {page}.")
                input("ðŸ”“ Vui lÃ²ng xá»­ lÃ½ CAPTCHA thá»§ cÃ´ng, sau Ä‘Ã³ nháº¥n Enter Ä‘á»ƒ tiáº¿p tá»¥c...")
            except NoSuchElementException:
                pass  # KhÃ´ng cÃ³ CAPTCHA, tiáº¿p tá»¥c bÃ¬nh thÆ°á»ng

            human_scroll(driver)
            # ActionChains(driver).move_by_offset(random.randint(0, 150), random.randint(0, 150)).perform()
            try:
                element = driver.find_element(By.TAG_NAME, "body")
                ActionChains(driver).move_to_element(element).perform()
            except Exception as e:
                print(f"âš ï¸ KhÃ´ng thá»ƒ move_to_element: {e}")

            random_sleep(1, 2)

            try:
                job_cards = wait.until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h3.title a'))
                )
            except (NoSuchElementException, TimeoutException) as e:
                print(f"âŒ KhÃ´ng tÃ¬m tháº¥y job cards trÃªn trang {page}: {e}")
                page += 1
                continue

            print(f"ðŸ“Œ TÃ¬m tháº¥y {len(job_cards)} job URLs trÃªn trang {page}")
            for job_card in job_cards:
                job_url = job_card.get_attribute("href")
                if job_url and job_url not in job_urls:
                    job_urls.append(job_url)

            page += 1
            random_sleep(1, 2)

            # TÃ¡i khá»Ÿi Ä‘á»™ng trÃ¬nh duyá»‡t sau má»—i 10 trang
            if page % 10 == 0:
                print("ðŸ” Äang khá»Ÿi Ä‘á»™ng láº¡i trÃ¬nh duyá»‡t Ä‘á»ƒ trÃ¡nh crash...")
                driver.quit()
                driver = init_driver()
                wait = WebDriverWait(driver, 20)
                random_sleep(2, 3)

    except WebDriverException as e:
        print(f"âŒ WebDriverException: {e}")
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {e}")
    finally:
        try:
            driver.quit()
        except:
            pass

    # LÆ°u káº¿t quáº£
    print(f"\nðŸ“¦ Tá»•ng sá»‘ URL thu tháº­p Ä‘Æ°á»£c: {len(job_urls)}")
    if job_urls:
        today = datetime.today().strftime("%Y-%m-%d")
        os.makedirs("../job_urls", exist_ok=True)
        filename = f"job_urls/topcv_{today}.txt"
        with open(filename, "w", encoding="utf-8") as f:
            for url in job_urls:
                f.write(url + "\n")
        print(f"âœ… ÄÃ£ lÆ°u vÃ o '{filename}'")
    else:
        print("âš ï¸ KhÃ´ng cÃ³ URL nÃ o Ä‘Æ°á»£c lÆ°u.")

if __name__ == "__main__":
    BASE_URL = "https://www.topcv.vn/tim-viec-lam-moi-nhat?type_keyword=0&sba=1"
    crawl_urls(BASE_URL, max_pages=200)
